{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TL;DR**: The following code walks through a step-by-step implementation of vector addition and uses Triton primitives such as `tl.arange`、`tl.load`、`tl.store`、`tl.program_id` and `tl.constexpr`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on Triton kernel implementation: Vector Addition\n",
    "\n",
    "Vector addition is the \"hello world\" program for Triton, can be written as:\n",
    "\n",
    "$$\n",
    "c = a + b\n",
    "$$\n",
    "\n",
    "$a$, $b$ and $c$ are all 1D tensors (vectors) of the same shape.\n",
    "\n",
    "### 1. PyTorch's built-in vector addition\n",
    "\n",
    "The built-in vector addition of PyTorch can be done simply using `+`, because it overrides the `+`operator.\n",
    "\n",
    "We’ll start with a tensor of shape torch.Size([16]).\n",
    "(Why not `15` or `17`? Is there a reason for choosing `16`? The short answer is yes—we’ll find out why soon.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1760682361402,
     "user": {
      "displayName": "tingxi li",
      "userId": "07148518068737199447"
     },
     "user_tz": 300
    },
    "id": "CaKlUGln2LlC",
    "outputId": "5906b8f6-82e2-45e7-a780-e3291159483d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3327, -0.3802,  0.0867,  0.1939,  0.4104, -0.9106, -1.0601, -0.1699,\n",
      "         0.4178, -1.8070, -1.0283,  0.2256,  0.2209, -1.0756,  0.1709, -0.6684],\n",
      "       device='cuda:0')\n",
      "tensor([ 0.7863,  0.1051,  0.0466, -0.1470,  1.3219,  0.5543, -0.5274,  0.7996,\n",
      "         1.1139, -0.1291, -1.2053,  1.1623, -1.4873, -0.4576,  1.1796, -0.7119],\n",
      "       device='cuda:0')\n",
      "tensor([ 0.4535, -0.2751,  0.1332,  0.0469,  1.7324, -0.3563, -1.5874,  0.6298,\n",
      "         1.5317, -1.9362, -2.2336,  1.3879, -1.2665, -1.5332,  1.3505, -1.3803],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "N = 16\n",
    "a = torch.randn(N, device='cuda')\n",
    "b = torch.randn(N, device='cuda')\n",
    "c = a + b\n",
    "print(a, b, c, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the $i$-th element in $c$ equals to the sum of the $i$-th element in $a$ and the $i$-th element in $b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The built-in vector addition in Pytorch calls the `vectorized_elementwise_kernel` CUDA kernel defined here: [aten/src/ATen/native/cuda/CUDALoops.cuh:L334](https://github.com/pytorch/pytorch/blob/ba56102387ef21a3b04b357e5b183d48f0afefc7/aten/src/ATen/native/cuda/CUDALoops.cuh#L334)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Triton 1D tensor addition in one Thread Block (program instance)\n",
    "\n",
    "**Thread Block and its max tensor numel( )**\n",
    "\n",
    "Unlike PyTorch, which can handle inputs of various shapes and sizes automatically, working at the lower level is non-trivial—our hardware cannot process arbitrarily large inputs. In Triton, you need to look more closely at what happens under the hood and exercise fine-grained control over memory allocation.\n",
    "\n",
    "That said, large input tensors must be handled differently and in a more complex way. It turns out that Triton prevents users from creating too many elements in a Thread Block, not because of a hardware limit, but as a (conservative) safeguard to avoid memory/complexity blowing up during compile-time. To start simply, we can use a smaller input size—`16` seems like a good choice within the capacity of one single Thread Block.\n",
    "\n",
    "More details of [Grid Bloack](https://modal.com/gpu-glossary/device-software/thread-block-grid), [Thread Block](https://modal.com/gpu-glossary/device-software/thread-block), [Warp](https://modal.com/gpu-glossary/device-software/warp) and [Thread](https://modal.com/gpu-glossary/device-software/thread).\n",
    "\n",
    "**Thread Block and Program Instance**\n",
    "\n",
    "In Triton, a program instance represents the [SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data)-level “instruction stream” (the “I”), and its lanes are the data lanes. In the context of NVIDIA GPUs,  executes those lanes using [SIMT](https://en.wikipedia.org/wiki/Single_instruction,_multiple_threads) threads, making the CUDA thread block the physical carrier of the “T”.\n",
    "\n",
    "Program instance $\\approx$ Thread Block, they are the same thing but from different perspectives.\n",
    "\n",
    "**Decorator**\n",
    "\n",
    "Triton kernel code must be wrapped in the [decorator](https://www.geeksforgeeks.org/system-design/decorator-pattern/) `@triton.jit` at its declaration. This decorator labels the function as a Triton kernel, allowing it to be Just-In-Time (JIT) compiled and launched as a specialized GPU program.\n",
    "\n",
    "Noted that we use pointers in Triton instead of the Pythonic reference when passing tensor parameters to a function. That's mainly because a Triton kernel is compiled to run on the GPU, and the GPU hardware operates on memory addresses (pointers) to access data efficiently in global memory. The pointers provide the necessary low-level control for the compiler to generate high-performance, hardware-aware code.\n",
    "\n",
    "In case you want to ask: How does the reference become a pointer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1760682371249,
     "user": {
      "displayName": "tingxi li",
      "userId": "07148518068737199447"
     },
     "user_tz": 300
    },
    "id": "QHGOCT9L2MAK",
    "outputId": "67fc2afd-8e48-43e1-eb9c-39481bec0012"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140081249648640\n"
     ]
    }
   ],
   "source": [
    "print(a.data_ptr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay now lets get back to the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1760682384281,
     "user": {
      "displayName": "tingxi li",
      "userId": "07148518068737199447"
     },
     "user_tz": 300
    },
    "id": "d_Wq420w2Oe1"
   },
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def vector_add_kernel(a_ptr, b_ptr, c_ptr):\n",
    "    pass\n",
    "\n",
    "def solve(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, N: int):\n",
    "    grid = (1,) # `1` denotes the number of blocks we used, for a 16-element vector, 1 is more than enough\n",
    "    vector_add_kernel[grid](a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As aforementioned, we focus on `N=16`, therefore, we need to pair the values in the same position of two vectors and perform element-wise addition. We can first use `tl.arange()` to generate indexes for elements [0, 1, 2, ..., 15]. The address of each element then can be written as `a_ptr + offset`. We then perform the addtion and save the result by using `tl.store( )`.\n",
    "\n",
    "There we have written the vector addition kernel in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1760682405035,
     "user": {
      "displayName": "tingxi li",
      "userId": "07148518068737199447"
     },
     "user_tz": 300
    },
    "id": "MPgS9tkV2RKj"
   },
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def vector_add_kernel(a_ptr, b_ptr, c_ptr):\n",
    "    offsets = tl.arange(0, 16) # generates indexes [0, 1, ..., 15] for 16 elements\n",
    "\n",
    "    a = tl.load(a_ptr + offsets) # load values from memory with their indexes, variable `a` and `b` are a Triton tensor, \n",
    "    b = tl.load(b_ptr + offsets) # more specifically, the datatype of `a` and `b` are tl.tensor(float32, (16,))\n",
    "\n",
    "    c = a + b # element wise addition\n",
    "\n",
    "    tl.store(c_ptr + offsets, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you might be curious about why the `+` operator works for `tl.tensor` here, Triton actually overloads `+` to perform element-wise addition. Unlike normal Python code where an element-wise vector addition would typically involve a hidden Python for-loop (or loops), the implementation of Triton's overloaded operator performs the operation in a truly parallel and vectorized manner on the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first and foremost thing for the kernel is always its correctness. We can use `torch.allclose()` to compare our results with the official PyTorch implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 398,
     "status": "ok",
     "timestamp": 1760682424004,
     "user": {
      "displayName": "tingxi li",
      "userId": "07148518068737199447"
     },
     "user_tz": 300
    },
    "id": "pyG3CbRq2U_D",
    "outputId": "25fd6395-cd87-4654-b632-2e2360775993"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Triton and Torch match\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def vector_add_kernel(a_ptr, b_ptr, c_ptr):\n",
    "    offsets = tl.arange(0, 16)\n",
    "    a = tl.load(a_ptr + offsets)\n",
    "    b = tl.load(b_ptr + offsets)\n",
    "    c = a + b\n",
    "    tl.store(c_ptr + offsets, c)\n",
    "\n",
    "def solve(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, N: int):\n",
    "    grid = (1,)\n",
    "    vector_add_kernel[grid](a, b, c)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    N = 16\n",
    "    a = torch.randn(N, device='cuda')\n",
    "    b = torch.randn(N, device='cuda')\n",
    "    torch_output = a + b\n",
    "    triton_output = torch.empty_like(a)\n",
    "    solve(a, b , triton_output, N)\n",
    "    if torch.allclose(triton_output, torch_output):\n",
    "        print(\"✅ Triton and Torch match\")\n",
    "    else:\n",
    "        print(\"❌ Triton and Torch differ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Bounded element load/store with `mask`\n",
    "\n",
    "**Callback 1**: What if the input tensor has 15 elements? We can simply change `N=16` to `N=15` to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 84,
     "status": "ok",
     "timestamp": 1760682438074,
     "user": {
      "displayName": "tingxi li",
      "userId": "07148518068737199447"
     },
     "user_tz": 300
    },
    "id": "MLUUqNUz2bQv",
    "outputId": "57813e55-0215-4f34-e66f-fcf567149e0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Triton and Torch match\n"
     ]
    }
   ],
   "source": [
    "\"\"\" WARNING: FOLLOWING CODE SAMPLE DEMONSTRATES A WRONG PATTERN\"\"\"\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def vector_add_kernel(a_ptr, b_ptr, c_ptr):\n",
    "    offsets = tl.arange(0, 16)\n",
    "    a = tl.load(a_ptr + offsets)\n",
    "    b = tl.load(b_ptr + offsets)\n",
    "    c = a + b\n",
    "    tl.store(c_ptr + offsets, c)\n",
    "\n",
    "def solve(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, N: int):\n",
    "    grid = (1,)\n",
    "    vector_add_kernel[grid](a, b, c)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    N = 15 # <- the only line we edit\n",
    "    a = torch.randn(N, device='cuda')\n",
    "    b = torch.randn(N, device='cuda')\n",
    "    torch_output = a + b\n",
    "    triton_output = torch.empty_like(a)\n",
    "    solve(a, b , triton_output, N)\n",
    "    if torch.allclose(triton_output, torch_output):\n",
    "        print(\"✅ Triton and Torch match\")\n",
    "    else:\n",
    "        print(\"❌ Triton and Torch differ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Surprisingly (or not), it gives the output:\n",
    "\n",
    "```c\n",
    "✅ Triton and Torch match\n",
    "```\n",
    "\n",
    "Actually, it works for any value of `N` in the range of [1, 16]. But this is EXTREMELY DANGEROUS because you are writing a part of the memory you are NOT supposed to touch, please never do so.\n",
    "\n",
    "To avoid out-of-bounds memory load/store, the size of the vectors and the `tl.arange()` need to be consistent. Persumably we can change `tl.arange(0, 16)` to `tl.arange(0, 15)` when we have `N=15`. Let's give it a try:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 525
    },
    "executionInfo": {
     "elapsed": 76,
     "status": "error",
     "timestamp": 1760682446450,
     "user": {
      "displayName": "tingxi li",
      "userId": "07148518068737199447"
     },
     "user_tz": 300
    },
    "id": "VjLvTx6i2exl",
    "outputId": "3eff3576-fab7-4e4d-a38c-efab4a6b5cb8"
   },
   "outputs": [
    {
     "ename": "CompilationError",
     "evalue": "at 2:14:\ndef vector_add_kernel(a_ptr, b_ptr, c_ptr):\n    offsets = tl.arange(0, 15) # <- the line we edit\n              ^",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/triton/language/core.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m                              \"(`_semantic` argument must be provided outside of JIT functions.)\")\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/triton/language/core.py\u001b[0m in \u001b[0;36marange\u001b[0;34m(start, end, _semantic)\u001b[0m\n\u001b[1;32m   1653\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unwrap_if_constexpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1654\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_semantic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/triton/language/semantic.py\u001b[0m in \u001b[0;36marange\u001b[0;34m(self, start, end, ret_ty)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"arange's range must be a power of 2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: arange's range must be a power of 2",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mCompilationError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2533690700.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtorch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtriton_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtriton_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriton_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"✅ Triton and Torch match\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-2533690700.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(a, b, c, N)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mvector_add_kernel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mmemorizes\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \"\"\"\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0;31m# return cast(T, functools.partial(cast(Callable, self.run), grid=grid))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, grid, warmup, *args, **kwargs)\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;31m# compile the kernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m             \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mASTSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstexprs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m             \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m             \u001b[0mkernel_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             self._call_hook(knobs.runtime.jit_post_compile_hook, key, signature, device, constexprs, options, [attrs],\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(src, target, options)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0mmodule_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_module_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodegen_fns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mfilter_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py\u001b[0m in \u001b[0;36mmake_ir\u001b[0;34m(self, options, codegen_fns, module_map, context)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodegen_fns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcode_generator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mast_to_ttir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\n\u001b[0m\u001b[1;32m     84\u001b[0m                            module_map=module_map)\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCompilationError\u001b[0m: at 2:14:\ndef vector_add_kernel(a_ptr, b_ptr, c_ptr):\n    offsets = tl.arange(0, 15) # <- the line we edit\n              ^"
     ]
    }
   ],
   "source": [
    "\"\"\" WARNING: FOLLOWING CODE SAMPLE DEMONSTRATES A WRONG PATTERN\"\"\"\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def vector_add_kernel(a_ptr, b_ptr, c_ptr):\n",
    "    offsets = tl.arange(0, 15) # <- the line we edit\n",
    "    a = tl.load(a_ptr + offsets)\n",
    "    b = tl.load(b_ptr + offsets)\n",
    "    c = a + b\n",
    "    tl.store(c_ptr + offsets, c)\n",
    "\n",
    "def solve(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, N: int):\n",
    "    grid = (1,)\n",
    "    vector_add_kernel[grid](a, b, c)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    N = 15 # <- the line we edit\n",
    "    a = torch.randn(N, device='cuda')\n",
    "    b = torch.randn(N, device='cuda')\n",
    "    torch_output = a + b\n",
    "    triton_output = torch.empty_like(a)\n",
    "    solve(a, b , triton_output, N)\n",
    "    if torch.allclose(triton_output, torch_output):\n",
    "        print(\"✅ Triton and Torch match\")\n",
    "    else:\n",
    "        print(\"❌ Triton and Torch differ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This error message indicates that the range of `tl.arange()` cannot be an arbitrary number, but must be a power of 2. That's because many GPU architectures and memory units prefer power-of-two vector lengths, so Triton enforces it for better and easier alignment.\n",
    "\n",
    "Luckily, Triton provides a workaround for this: the `mask`. A `mask` is a Triton tensor filled with boolean values, which indicates which elements of the input vector are out-of-bounds. Here is an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Discussion Question**: \n",
    "\n",
    "In the code snippet above, the variable `N` is a runtime variable; however, Triton kernels require JIT compilation. Please explain: why doing `mask = offsets < N` works fine, but `tl.arange(0, N)` causes an error even when `N` is a power of two? Is there a way to write `tl.arange(0, N)` without errors?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current Triton kernel only works properly when the input size is ≤ 16. We want our kernel to handle inputs of any size, but we don’t want to create a kernel that uses an excessively large compile-time tile size, which would increase register usage and reduce occupancy. Nor do we want to define multiple kernels with different fixed tile sizes, compile them JIT, and select one at runtime — both approaches would waste valuable GPU resources. Instead, we want a templated GPU kernel that can adaptively and efficiently handle arbitrary input sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 473,
     "status": "ok",
     "timestamp": 1760682469724,
     "user": {
      "displayName": "tingxi li",
      "userId": "07148518068737199447"
     },
     "user_tz": 300
    },
    "id": "RCG5lKrR2g0k",
    "outputId": "53b611c1-9041-46b0-afe3-cadcb67e8729"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Triton and Torch match\n",
      "✅ Triton and Torch match\n",
      "✅ Triton and Torch match\n",
      "✅ Triton and Torch match\n",
      "✅ Triton and Torch match\n",
      "✅ Triton and Torch match\n",
      "✅ Triton and Torch match\n",
      "✅ Triton and Torch match\n",
      "✅ Triton and Torch match\n",
      "✅ Triton and Torch match\n",
      "✅ Triton and Torch match\n",
      "✅ Triton and Torch match\n",
      "✅ Triton and Torch match\n",
      "✅ Triton and Torch match\n",
      "✅ Triton and Torch match\n",
      "✅ Triton and Torch match\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def vector_add_kernel(a_ptr, b_ptr, c_ptr, N):\n",
    "    offsets = tl.arange(0, 16)\n",
    "\n",
    "    mask = offsets < N # size of your input vector\n",
    "\n",
    "    a = tl.load(a_ptr + offsets, mask=mask)\n",
    "    b = tl.load(b_ptr + offsets, mask=mask)\n",
    "    c = a + b\n",
    "    tl.store(c_ptr + offsets, c, mask=mask)\n",
    "\n",
    "def solve(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, N: int):\n",
    "    grid = (1,)\n",
    "    vector_add_kernel[grid](a, b, c, N)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for N in range(1, 16+1):\n",
    "        a = torch.randn(N, device='cuda')\n",
    "        b = torch.randn(N, device='cuda')\n",
    "        torch_output = a + b\n",
    "        triton_output = torch.empty_like(a)\n",
    "        solve(a, b , triton_output, N)\n",
    "        if torch.allclose(triton_output, torch_output):\n",
    "            print(\"✅ Triton and Torch match\")\n",
    "        else:\n",
    "            print(\"❌ Triton and Torch differ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Multiple Thread Blocks for Large-size Inputs\n",
    "\n",
    "**Callback 2**: The official documentation of `tl.arange()` says this function:\n",
    "\n",
    ">Returns contiguous values within the half-open interval [start, end). end - start must be less than or equal to TRITON_MAX_TENSOR_NUMEL = 1048576\n",
    "\n",
    "`1048576` (2^20) the max number of elements within one single Thread Block, so multiple Thread Blocks are needed. In Triton, the program instance (often parameterized by BLOCK) is the finest granularity of scheduling and optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Discussion Question**: \n",
    "\n",
    "In CUDA, you think in terms of threads and warps:\n",
    "\n",
    "```c\n",
    "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "float a = A[tid];\n",
    "float b = B[tid];\n",
    "C[tid] = a + b;\n",
    "```\n",
    "\n",
    "however, Triton let programmers to express the math and data layout at the tile level, and let the compiler to handle threads and warps. Please explain why warps and threads are hidden abstractions in Triton, and what benefits (and trade-offs) that gives you.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In NVIDIA GPU, the hierarchy of logical execution (from high level to low level) is:\n",
    "> Grid -- Thread Block -- Warp -- Thread\n",
    "\n",
    "Just like what we do with the offset in previous code blocks, we don't need to define the behavior of each block -- remember we have SIMT inside of a Thread Block? Similarily, we have SPMD (Single Program Multiple Data) here.\n",
    "\n",
    "A grid is a collection of blocks, and it can be 1D, 2D, or 3D, it can be defined as `grid = ()`, e.g. `grid=(B, C, H, W)`. For vector operations, it’s sufficient to arange the blocks only along the x dimension. Inside the kernel, we can use `tl.program_id(axis=0)` to obtain the block index.\n",
    "\n",
    "Therefore, multiple blocks can be managed by Grid. Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1670,
     "status": "ok",
     "timestamp": 1760682591687,
     "user": {
      "displayName": "tingxi li",
      "userId": "07148518068737199447"
     },
     "user_tz": 300
    },
    "id": "wPuKSu4V2mZ_",
    "outputId": "3452ff08-d6ba-44d4-efe8-43f705531c79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Triton and Torch match with input size 2^1\n",
      "Torch  time: 1.33550 ms, \n",
      "Triton time: 4.31417 ms\n",
      "grid size:  1048576 \n",
      "\n",
      "✅ Triton and Torch match with input size 2^3\n",
      "Torch  time: 1.61238 ms, \n",
      "Triton time: 3.68864 ms\n",
      "grid size:  1048576 \n",
      "\n",
      "✅ Triton and Torch match with input size 2^5\n",
      "Torch  time: 1.14208 ms, \n",
      "Triton time: 3.15389 ms\n",
      "grid size:  1048576 \n",
      "\n",
      "✅ Triton and Torch match with input size 2^7\n",
      "Torch  time: 1.25037 ms, \n",
      "Triton time: 2.98867 ms\n",
      "grid size:  1048576 \n",
      "\n",
      "✅ Triton and Torch match with input size 2^9\n",
      "Torch  time: 1.57355 ms, \n",
      "Triton time: 2.96891 ms\n",
      "grid size:  1048576 \n",
      "\n",
      "✅ Triton and Torch match with input size 2^11\n",
      "Torch  time: 1.12892 ms, \n",
      "Triton time: 2.86296 ms\n",
      "grid size:  1048576 \n",
      "\n",
      "✅ Triton and Torch match with input size 2^13\n",
      "Torch  time: 1.80937 ms, \n",
      "Triton time: 3.51826 ms\n",
      "grid size:  1048576 \n",
      "\n",
      "✅ Triton and Torch match with input size 2^15\n",
      "Torch  time: 1.25404 ms, \n",
      "Triton time: 2.88556 ms\n",
      "grid size:  1048576 \n",
      "\n",
      "✅ Triton and Torch match with input size 2^17\n",
      "Torch  time: 1.35573 ms, \n",
      "Triton time: 2.77837 ms\n",
      "grid size:  1048576 \n",
      "\n",
      "✅ Triton and Torch match with input size 2^19\n",
      "Torch  time: 1.52062 ms, \n",
      "Triton time: 2.86905 ms\n",
      "grid size:  1048576 \n",
      "\n",
      "✅ Triton and Torch match with input size 2^21\n",
      "Torch  time: 1.27100 ms, \n",
      "Triton time: 2.63440 ms\n",
      "grid size:  1048576 \n",
      "\n",
      "✅ Triton and Torch match with input size 2^23\n",
      "Torch  time: 1.25251 ms, \n",
      "Triton time: 3.44298 ms\n",
      "grid size:  1048576 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def vector_add_kernel(a_ptr, b_ptr, c_ptr, N):\n",
    "\n",
    "    pid = tl.program_id(axis=0) # pid is a unique ID for each Thread Block\n",
    "\n",
    "    block_start = pid * 16 # slicing data for each block\n",
    "\n",
    "    offsets = block_start + tl.arange(0, 16)\n",
    "    mask = offsets < N\n",
    "    a = tl.load(a_ptr + offsets, mask=mask)\n",
    "    b = tl.load(b_ptr + offsets, mask=mask)\n",
    "    c = a + b\n",
    "    tl.store(c_ptr + offsets, c, mask=mask)\n",
    "\n",
    "def solve(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, N: int):\n",
    "    grid = (triton.cdiv(N, 16), )\n",
    "    vector_add_kernel[grid](a, b, c, N)\n",
    "\n",
    "\n",
    "def time_op_gpu(fn, sync=True, warmup=5, iters=20):\n",
    "    \"\"\"\n",
    "    Time a GPU operation using CUDA events for better accuracy (no CPU scheduling noise).\n",
    "    - fn: a callable that launches GPU work\n",
    "    - sync: whether to synchronize after each iteration (True recommended)\n",
    "    - warmup: warm-up iterations to let JIT/caches settle\n",
    "    - iters: timed iterations\n",
    "\n",
    "    Returns: average time in milliseconds over 'iters' runs.\n",
    "    \"\"\"\n",
    "    # warm-up does JIT and warms caches\n",
    "    for _ in range(warmup):\n",
    "        fn()\n",
    "    if sync:\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    elapsed_ms = 0.0\n",
    "    for _ in range(iters):\n",
    "        start.record()\n",
    "        fn()\n",
    "        end.record()\n",
    "        # Wait for the events to be recorded & measure GPU time\n",
    "        torch.cuda.synchronize()\n",
    "        elapsed_ms += start.elapsed_time(end)\n",
    "    return elapsed_ms / iters\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for power in range(1, 25 ,2):\n",
    "        N = 2 ** power\n",
    "        N = 1 << 24\n",
    "        a = torch.randn(N, device='cuda')\n",
    "        b = torch.randn(N, device='cuda')\n",
    "        triton_output = torch.empty_like(a)\n",
    "\n",
    "        def torch_op():\n",
    "            return a + b\n",
    "\n",
    "        def triton_op():\n",
    "            triton_output = torch.empty_like(a)\n",
    "            solve(a, b, triton_output, N)\n",
    "            return triton_output\n",
    "\n",
    "        torch_output = torch_op()  # warm-up\n",
    "        torch_time_elapsed = time_op_gpu(torch_op)\n",
    "\n",
    "        triton_output = triton_op()  # warm-up\n",
    "        triton_time_elapsed = time_op_gpu(triton_op)\n",
    "\n",
    "        if torch.allclose(triton_output, torch_output):\n",
    "            print(f\"✅ Triton and Torch match with input size 2^{power}\")\n",
    "            print(f\"Torch  time: {torch_time_elapsed:.5f} ms, \\nTriton time: {triton_time_elapsed:.5f} ms\")\n",
    "        else:\n",
    "            print(f\"❌ Triton and Torch differ with input size 2^{power}\")\n",
    "\n",
    "        print(\"grid size: \", triton.cdiv(N, 16), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, it passes all the tests! But looking at the printed output:\n",
    "\n",
    "\n",
    "Well we observe that Triton kernel is SLOWER than the PyTorch Implementation. Why is that?\n",
    "\n",
    "Short answer: the Triton kernel is **under-utilizing** the GPU and paying proportionally higher fixed overhead than PyTorch’s highly-tuned kernel.\n",
    "\n",
    "Here is a detailed breakdown:\n",
    "\n",
    "1. We launch `N` blocks, while each of them does only 16 additions, so the prologue/epilogue overhead per block dominates the time complexity.\n",
    "2. PyTorch's vector addition, even as a fallback, it is not bad at all. It is actually heavily optimized ($O(1)$ launch complexity) especially for small input size with minimum launch overhead, while Triton is more time-consuming to launch and compile.\n",
    "3. By default, 4 warps (4 * 32 = 128 threads) will be assigned to a block at launch, even if you don't use them, registers are still allocated and reserved for them, leads to a low occupancy.\n",
    "\n",
    "An intuitive solution to this is: we want a number larger than `16` in the line:\n",
    "\n",
    "```python\n",
    "offsets = block_start + tl.arange(0, 16)\n",
    "```\n",
    "\n",
    "or, in a even better way, we want to write:\n",
    "\n",
    "```python\n",
    "offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "```\n",
    "\n",
    "where `BLOCK_SIZE` is a runtime constant. There we have the final version of our vector addition kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 257,
     "status": "ok",
     "timestamp": 1760682638427,
     "user": {
      "displayName": "tingxi li",
      "userId": "07148518068737199447"
     },
     "user_tz": 300
    },
    "id": "L2quNyzi2qrS",
    "outputId": "df7ab327-5055-40ac-99b2-19147d69e141"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Triton and Torch match with input size 2^1\n",
      "Torch  time: 0.17645 ms, \n",
      "Triton time: 0.09037 ms\n",
      "grid size:  1 \n",
      "\n",
      "✅ Triton and Torch match with input size 2^3\n",
      "Torch  time: 0.04901 ms, \n",
      "Triton time: 0.18940 ms\n",
      "grid size:  1 \n",
      "\n",
      "✅ Triton and Torch match with input size 2^5\n",
      "Torch  time: 0.31917 ms, \n",
      "Triton time: 0.07534 ms\n",
      "grid size:  2 \n",
      "\n",
      "✅ Triton and Torch match with input size 2^7\n",
      "Torch  time: 0.34054 ms, \n",
      "Triton time: 0.06917 ms\n",
      "grid size:  8 \n",
      "\n",
      "✅ Triton and Torch match with input size 2^9\n",
      "Torch  time: 0.04664 ms, \n",
      "Triton time: 0.19412 ms\n",
      "grid size:  32 \n",
      "\n",
      "✅ Triton and Torch match with input size 2^11\n",
      "Torch  time: 0.07051 ms, \n",
      "Triton time: 0.06189 ms\n",
      "grid size:  128 \n",
      "\n",
      "✅ Triton and Torch match with input size 2^13\n",
      "Torch  time: 0.23926 ms, \n",
      "Triton time: 0.08479 ms\n",
      "grid size:  512 \n",
      "\n",
      "✅ Triton and Torch match with input size 2^15\n",
      "Torch  time: 0.07361 ms, \n",
      "Triton time: 0.07344 ms\n",
      "grid size:  2048 \n",
      "\n",
      "✅ Triton and Torch match with input size 2^17\n",
      "Torch  time: 0.05223 ms, \n",
      "Triton time: 0.06151 ms\n",
      "grid size:  8192 \n",
      "\n",
      "✅ Triton and Torch match with input size 2^19\n",
      "Torch  time: 0.25605 ms, \n",
      "Triton time: 0.08298 ms\n",
      "grid size:  32768 \n",
      "\n",
      "✅ Triton and Torch match with input size 2^21\n",
      "Torch  time: 0.76483 ms, \n",
      "Triton time: 0.46037 ms\n",
      "grid size:  131072 \n",
      "\n",
      "✅ Triton and Torch match with input size 2^23\n",
      "Torch  time: 1.53014 ms, \n",
      "Triton time: 0.63391 ms\n",
      "grid size:  524288 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def vector_add_kernel(a_ptr, b_ptr, c_ptr, N, BLOCK: tl.constexpr):\n",
    "\n",
    "    pid = tl.program_id(axis=0) # pid is a unique ID for each Thread Block\n",
    "\n",
    "    # block_start = pid * 16 # slicing data for each block\n",
    "    # offsets = block_start + tl.arange(0, 16)\n",
    "\n",
    "    offsets = pid * BLOCK + tl.arange(0, BLOCK)\n",
    "\n",
    "    mask = offsets < N\n",
    "    a = tl.load(a_ptr + offsets, mask=mask)\n",
    "    b = tl.load(b_ptr + offsets, mask=mask)\n",
    "    c = a + b\n",
    "    tl.store(c_ptr + offsets, c, mask=mask)\n",
    "\n",
    "def solve(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, N: int):\n",
    "    BLOCK = 1024\n",
    "    grid = (triton.cdiv(N, BLOCK), )\n",
    "    vector_add_kernel[grid](a, b, c, N, BLOCK=BLOCK, num_warps=4)\n",
    "\n",
    "\n",
    "def time_op_gpu(fn, sync=True, warmup=5, iters=20):\n",
    "    \"\"\"\n",
    "    Time a GPU operation using CUDA events for better accuracy (no CPU scheduling noise).\n",
    "    - fn: a callable that launches GPU work\n",
    "    - sync: whether to synchronize after each iteration (True recommended)\n",
    "    - warmup: warm-up iterations to let JIT/caches settle\n",
    "    - iters: timed iterations\n",
    "\n",
    "    Returns: average time in milliseconds over 'iters' runs.\n",
    "    \"\"\"\n",
    "    # warm-up does JIT and warms caches\n",
    "    for _ in range(warmup):\n",
    "        fn()\n",
    "    if sync:\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    elapsed_ms = 0.0\n",
    "    for _ in range(iters):\n",
    "        start.record()\n",
    "        fn()\n",
    "        end.record()\n",
    "        # Wait for the events to be recorded & measure GPU time\n",
    "        torch.cuda.synchronize()\n",
    "        elapsed_ms += start.elapsed_time(end)\n",
    "    return elapsed_ms / iters\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for power in range(1, 25, 2):\n",
    "        N = 2 ** power\n",
    "        a = torch.randn(N, device='cuda')\n",
    "        b = torch.randn(N, device='cuda')\n",
    "        torch_output  = torch.empty_like(a)\n",
    "        triton_output = torch.empty_like(a)\n",
    "\n",
    "        def torch_op():\n",
    "            return torch_output.copy_(a + b)\n",
    "\n",
    "        def triton_op():\n",
    "            triton_output = torch.empty_like(a)\n",
    "            solve(a, b, triton_output, N)\n",
    "            return triton_output\n",
    "\n",
    "        torch_output = torch_op()  # warm-up\n",
    "        torch_time_elapsed = time_op_gpu(torch_op)\n",
    "\n",
    "        triton_output = triton_op()  # warm-up\n",
    "        triton_time_elapsed = time_op_gpu(triton_op)\n",
    "\n",
    "        if torch.allclose(triton_output, torch_output):\n",
    "            print(f\"✅ Triton and Torch match with input size 2^{power}\")\n",
    "            print(f\"Torch  time: {torch_time_elapsed:.5f} ms, \\nTriton time: {triton_time_elapsed:.5f} ms\")\n",
    "        else:\n",
    "            print(f\"❌ Triton and Torch differ with input size 2^{power}\")\n",
    "\n",
    "        print(\"grid size: \", triton.cdiv(N, 16), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha! Triton is FASTER, when we have large input sizes!\n",
    "\n",
    "---\n",
    "\n",
    "**Discussion Question**:\n",
    "\n",
    "1. Fixed `N`, Varying `BLOCK_SIZE`: With a fixed input size `N`, how does changing the `BLOCK_SIZE` affect the kernel's efficiency, and what is the optimal `BLOCK_SIZE` for the target hardware?\n",
    "2. Fixed `BLOCK_SIZE`, Varying `N`: With a fixed `BLOCK_SIZE`, how does changing the input size `N` affect the kernel's efficiency, and what is the optimal `N` that maximizes efficiency for the target hardware?\n",
    "3. Do your observations and conclusions from questions 1 and 2 vary across different GPU architectures or hardware?\n",
    "\n",
    "---\n",
    "\n",
    "[LeetGPU](https://leetgpu.com) is the leetcode for GPU kernel programming, it offers test cases and different hardwares for you to evaluate the code you write. Check this link out: [Vector Addition](https://leetgpu.com/challenges/vector-addition), and submit your code to see if it passes the test.\n",
    "\n",
    "![submit code to LeetGPU - Vector Addition](https://img2024.cnblogs.com/blog/1154439/202508/1154439-20250831151106589-1307418925.png)\n",
    "\n",
    "### 6. Source Code\n",
    "\n",
    "Source code of this markdown can be found in `./ex1-vector_add-en/vector_add.ipynb`\n",
    "\n",
    "## Beyond Vector Addition\n",
    "\n",
    "More practice:\n",
    "\n",
    "[Matrix Copy](https://leetgpu.com/challenges/matrix-copy)\n",
    "\n",
    "[Color Inversion](https://leetgpu.com/challenges/color-inversion) \n",
    "\n",
    "[Reverse Array](https://leetgpu.com/challenges/reverse-array) \n",
    "\n",
    "[Matrix Transpose](https://leetgpu.com/challenges/matrix-transpose) \n",
    "\n",
    "[ReLU](https://leetgpu.com/challenges/relu) \n",
    "\n",
    "[Leaky ReLU](https://leetgpu.com/challenges/leaky-relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOneuniVD62RA+3s4eS4edo",
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
